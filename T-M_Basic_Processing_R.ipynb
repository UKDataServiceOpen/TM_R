{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UKDS Logo](images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-mining: Basics in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the UK Data Service training series on New Forms of Data for Social Science Research. This series guides you through some of the most common and valuable new sources of data available for social science research: data collected from websites, social media platorms, text data, conducting simulations (agent based modelling), to name a few. We provide webinars, interactive notebooks containing live programming code, reading lists and more.\n",
    "\n",
    "To access training materials for the entire series: [Training Materials]\n",
    "\n",
    "To keep up to date with upcoming and past training events: [Events]\n",
    "\n",
    "To get in contact with feedback, ideas or to seek assistance: [Help]\n",
    "\n",
    "Dr Julia Kasmire\n",
    "UK Data Service\n",
    "University of Manchester\n",
    "November 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "1  Introduction\n",
    "2  Retrieval\n",
    "3  Processing\n",
    "3.1  Tokenisation\n",
    "3.2  Standardising\n",
    "3.2.1  Remove uppercase letters\n",
    "3.2.2  Spelling correction\n",
    "3.2.3  RegEx replacements\n",
    "3.3  Removing irrelevancies\n",
    "3.3.1  Remove punctuation\n",
    "3.3.2  Stopwords\n",
    "3.4  Consolidation\n",
    "3.4.1  Stemming words\n",
    "3.4.2  Lemmatisation\n",
    "4  Conclusions\n",
    "4.1  Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a table of contents provided here at the top of the notebook, but you can also access this menu at any point by clicking the Table of Contents button on the top toolbar (an icon with four horizontal bars, if unsure hover your mouse over the buttons). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first in a series of jupyter notebooks on text-mining that cover basic preparation processes, common natural language processing tasks, and some more advanced natural language tasks. These interactive code-along notebooks use python as a programming language, but introduce various packages related to text-mining and text processing. Most of those tasks could be done in other packages, so please be aware that the options demonstrated here are not the only way, or even the best way, to accomplish a text-mining task. \n",
    "\n",
    "For more information on what jupyter notebooks are or how to interact with them, follow [THESE LINKS] (insert links please). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in text-mining, or any form of data-mining, is retrieving a data set to work with. Within text-mining, or any language analysis context, one data set is usually referred to as 'a corpus' while multiple data sets are referred to as 'corpora'. 'Corpus' is a latin-root word and therefore has a funny plural. \n",
    "\n",
    "For text-mining, a corpus can be:\n",
    "- a set of tweets, \n",
    "- the full text of an 18th centrury novel,\n",
    "- the contents of a page in the dictionary, \n",
    "- minutes of local council meetings, \n",
    "- random gibberish letters and numbers, or\n",
    "- just about anything else in text format. \n",
    "\n",
    "\n",
    "Retrieval is a very important step, but it is not the focus of this particular training series. If you are interested in creating a corpus from internet data, then you may want to check out previous the NFoD training series that covers Web-scraping (available as recordings of webinars or as a code-along jupyter notebook like this one) and API's (also as recording or jupyter notebook). Both of these demonstrate and discuss ways to get data from the internet that you could use to build a corpus. \n",
    "\n",
    "Instead, for the purposes of this session, we will assume that you already have a corpus to analyse. This is easy for us to assume, because we have provided a sample text file that we can use as a corpus for these exercises. \n",
    "\n",
    "First, let's check that it is there. To do that, click in the code cell below and hit the 'Run' button at the top of this page or by holding down the 'Shift' key and hitting the 'Enter' key. \n",
    "\n",
    "For the rest of this notebook, I will use 'Run/Shift+Enter' as short hand for 'click in the code cell below and hit the 'Run' button at the top of this page or by hold down the 'Shift' key while hitting the 'Enter' key'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1. Succesfully imported necessary modules\"\n"
     ]
    }
   ],
   "source": [
    "# It is good practice to always start by importing the modules and packages you will need. \n",
    "\n",
    "library(dplyr)\n",
    "\n",
    "print(\"1. Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character(0)\n",
      " [1] \"$RECYCLE.BIN\" \"Desktop\"      \"desktop.ini\"  \"Documents\"    \"Downloads\"   \n",
      " [6] \"Favorites\"    \"My Music\"     \"My Pictures\"  \"My Videos\"    \"R\"           \n"
     ]
    }
   ],
   "source": [
    "print(list.files(path = \"./data\", pattern = \"*\"))      # create list of all .csv files in folder\n",
    "\n",
    "print(list.files(path = \"./\", pattern = \"*\"))      # create list of all .csv files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "Great! We have imported a useful module and used it to check that we have access to the sample_text file. \n",
    "\n",
    "Now we need to load that sample_text file into a variable that we can work with in python. Time to Run/Shift+Enter again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the \"sample_text\" file and read (import) its contents to a variable called \"corpus\"\n",
    "with open(\"./data/sample_text.txt\", \"r\") as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "    print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Hmm. Not excellent literature, but it will do for our purposes. \n",
    "\n",
    "A quick look tells us that there are capital letters, contractions, punctuation, numbers as digits, numbers written out, abbreviations, and other things that, as humans, we know are equivalent but that computers do not know about. \n",
    "\n",
    "Before we go further, it helps to know what kind of variable corpus is. Run/Shift+Enter the next code block to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "This tells us that 'corpus' is one very long string of text characters.  \n",
    "\n",
    "Congratulations! We are done with the retreival portion of this process. The rest won't be quite so straightforward because next up... Processing. \n",
    "\n",
    "Processing is about cleaning, correcting, standardizing and formatting the raw data returned from the retrieval process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove uppercase letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegEx replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrelevancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmitisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
